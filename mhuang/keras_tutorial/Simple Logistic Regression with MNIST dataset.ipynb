{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train_raw, Y_train_raw), (X_test_raw, Y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data from image matrix to array & normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train_raw.reshape(X_train_raw.shape[0], X_train_raw.shape[1]*X_train_raw.shape[2]).astype('float32')\n",
    "X_train /= 255.\n",
    "\n",
    "X_test = X_test_raw.reshape(X_test_raw.shape[0], X_test_raw.shape[1]*X_test_raw.shape[2]).astype('float32')\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train_raw)\n",
    "\n",
    "Y_test = np_utils.to_categorical(Y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 10)            7850        dense_input_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 7850\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.7782 - acc: 0.8177 - val_loss: 0.4723 - val_acc: 0.8848\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.4609 - acc: 0.8788 - val_loss: 0.3957 - val_acc: 0.8995\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.4076 - acc: 0.8895 - val_loss: 0.3656 - val_acc: 0.9033\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3807 - acc: 0.8956 - val_loss: 0.3477 - val_acc: 0.9069\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3640 - acc: 0.8994 - val_loss: 0.3355 - val_acc: 0.9101\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3519 - acc: 0.9031 - val_loss: 0.3269 - val_acc: 0.9105\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3427 - acc: 0.9041 - val_loss: 0.3207 - val_acc: 0.9117\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3355 - acc: 0.9066 - val_loss: 0.3147 - val_acc: 0.9140\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3295 - acc: 0.9077 - val_loss: 0.3103 - val_acc: 0.9158\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3245 - acc: 0.9095 - val_loss: 0.3069 - val_acc: 0.9152\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3202 - acc: 0.9106 - val_loss: 0.3039 - val_acc: 0.9162\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3164 - acc: 0.9119 - val_loss: 0.3013 - val_acc: 0.9162\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3131 - acc: 0.9129 - val_loss: 0.2993 - val_acc: 0.9167\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3102 - acc: 0.9138 - val_loss: 0.2967 - val_acc: 0.9176\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3076 - acc: 0.9144 - val_loss: 0.2944 - val_acc: 0.9181\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2931 - val_acc: 0.9195\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3029 - acc: 0.9157 - val_loss: 0.2917 - val_acc: 0.9194\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.3011 - acc: 0.9162 - val_loss: 0.2898 - val_acc: 0.9198\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2989 - acc: 0.9169 - val_loss: 0.2884 - val_acc: 0.9205\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2974 - acc: 0.9169 - val_loss: 0.2876 - val_acc: 0.9199\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2958 - acc: 0.9179 - val_loss: 0.2868 - val_acc: 0.9202\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2943 - acc: 0.9186 - val_loss: 0.2854 - val_acc: 0.9209\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2929 - acc: 0.9183 - val_loss: 0.2846 - val_acc: 0.9208\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2915 - acc: 0.9192 - val_loss: 0.2838 - val_acc: 0.9212\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2904 - acc: 0.9193 - val_loss: 0.2828 - val_acc: 0.9218\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2892 - acc: 0.9194 - val_loss: 0.2821 - val_acc: 0.9217\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2881 - acc: 0.9199 - val_loss: 0.2814 - val_acc: 0.9216\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2870 - acc: 0.9203 - val_loss: 0.2807 - val_acc: 0.9228\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2860 - acc: 0.9206 - val_loss: 0.2802 - val_acc: 0.9227\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2850 - acc: 0.9206 - val_loss: 0.2798 - val_acc: 0.9219\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2842 - acc: 0.9212 - val_loss: 0.2797 - val_acc: 0.9216\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2834 - acc: 0.9214 - val_loss: 0.2785 - val_acc: 0.9237\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2825 - acc: 0.9213 - val_loss: 0.2779 - val_acc: 0.9231\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2817 - acc: 0.9219 - val_loss: 0.2773 - val_acc: 0.9232\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2809 - acc: 0.9218 - val_loss: 0.2770 - val_acc: 0.9230\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2802 - acc: 0.9218 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2795 - acc: 0.9221 - val_loss: 0.2768 - val_acc: 0.9227\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2787 - acc: 0.9228 - val_loss: 0.2762 - val_acc: 0.9237\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2781 - acc: 0.9227 - val_loss: 0.2757 - val_acc: 0.9232\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2776 - acc: 0.9226 - val_loss: 0.2753 - val_acc: 0.9222\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2768 - acc: 0.9229 - val_loss: 0.2751 - val_acc: 0.9237\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2764 - acc: 0.9234 - val_loss: 0.2744 - val_acc: 0.9238\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2758 - acc: 0.9233 - val_loss: 0.2741 - val_acc: 0.9230\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2752 - acc: 0.9236 - val_loss: 0.2740 - val_acc: 0.9234\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2748 - acc: 0.9237 - val_loss: 0.2733 - val_acc: 0.9242\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2742 - acc: 0.9240 - val_loss: 0.2730 - val_acc: 0.9246\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2736 - acc: 0.9236 - val_loss: 0.2729 - val_acc: 0.9241\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2732 - acc: 0.9239 - val_loss: 0.2730 - val_acc: 0.9238\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2727 - acc: 0.9243 - val_loss: 0.2727 - val_acc: 0.9241\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2722 - acc: 0.9243 - val_loss: 0.2721 - val_acc: 0.9237\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2718 - acc: 0.9245 - val_loss: 0.2726 - val_acc: 0.9247\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2714 - acc: 0.9244 - val_loss: 0.2720 - val_acc: 0.9237\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2709 - acc: 0.9247 - val_loss: 0.2716 - val_acc: 0.9240\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2705 - acc: 0.9250 - val_loss: 0.2718 - val_acc: 0.9244\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2701 - acc: 0.9251 - val_loss: 0.2714 - val_acc: 0.9243\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2697 - acc: 0.9251 - val_loss: 0.2712 - val_acc: 0.9253\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2694 - acc: 0.9248 - val_loss: 0.2710 - val_acc: 0.9245\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2689 - acc: 0.9251 - val_loss: 0.2707 - val_acc: 0.9249\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2685 - acc: 0.9252 - val_loss: 0.2712 - val_acc: 0.9251\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2681 - acc: 0.9256 - val_loss: 0.2705 - val_acc: 0.9243\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2678 - acc: 0.9255 - val_loss: 0.2705 - val_acc: 0.9252\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2675 - acc: 0.9252 - val_loss: 0.2702 - val_acc: 0.9253\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2672 - acc: 0.9256 - val_loss: 0.2700 - val_acc: 0.9253\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2668 - acc: 0.9253 - val_loss: 0.2703 - val_acc: 0.9251\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2665 - acc: 0.9257 - val_loss: 0.2698 - val_acc: 0.9256\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2661 - acc: 0.9260 - val_loss: 0.2699 - val_acc: 0.9253\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2659 - acc: 0.9259 - val_loss: 0.2695 - val_acc: 0.9261\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2655 - acc: 0.9261 - val_loss: 0.2694 - val_acc: 0.9253\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2653 - acc: 0.9262 - val_loss: 0.2691 - val_acc: 0.9256\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2650 - acc: 0.9260 - val_loss: 0.2689 - val_acc: 0.9269\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2647 - acc: 0.9261 - val_loss: 0.2692 - val_acc: 0.9257\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2644 - acc: 0.9264 - val_loss: 0.2686 - val_acc: 0.9263\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2641 - acc: 0.9264 - val_loss: 0.2693 - val_acc: 0.9256\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2639 - acc: 0.9264 - val_loss: 0.2686 - val_acc: 0.9263\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2635 - acc: 0.9268 - val_loss: 0.2687 - val_acc: 0.9260\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2634 - acc: 0.9261 - val_loss: 0.2686 - val_acc: 0.9258\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2630 - acc: 0.9264 - val_loss: 0.2688 - val_acc: 0.9266\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2628 - acc: 0.9269 - val_loss: 0.2682 - val_acc: 0.9264\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2625 - acc: 0.9271 - val_loss: 0.2680 - val_acc: 0.9263\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2623 - acc: 0.9270 - val_loss: 0.2682 - val_acc: 0.9268\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2620 - acc: 0.9269 - val_loss: 0.2680 - val_acc: 0.9263\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2619 - acc: 0.9268 - val_loss: 0.2675 - val_acc: 0.9269\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2615 - acc: 0.9271 - val_loss: 0.2676 - val_acc: 0.9263\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2612 - acc: 0.9273 - val_loss: 0.2682 - val_acc: 0.9258\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2610 - acc: 0.9270 - val_loss: 0.2682 - val_acc: 0.9261\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2608 - acc: 0.9268 - val_loss: 0.2677 - val_acc: 0.9267\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2606 - acc: 0.9274 - val_loss: 0.2671 - val_acc: 0.9271\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2603 - acc: 0.9275 - val_loss: 0.2673 - val_acc: 0.9273\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2602 - acc: 0.9271 - val_loss: 0.2677 - val_acc: 0.9261\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2599 - acc: 0.9275 - val_loss: 0.2678 - val_acc: 0.9258\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2598 - acc: 0.9274 - val_loss: 0.2675 - val_acc: 0.9268\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2596 - acc: 0.9279 - val_loss: 0.2672 - val_acc: 0.9266\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2593 - acc: 0.9276 - val_loss: 0.2678 - val_acc: 0.9276\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2591 - acc: 0.9276 - val_loss: 0.2675 - val_acc: 0.9269\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2589 - acc: 0.9278 - val_loss: 0.2672 - val_acc: 0.9267\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2587 - acc: 0.9282 - val_loss: 0.2667 - val_acc: 0.9273\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2585 - acc: 0.9281 - val_loss: 0.2665 - val_acc: 0.9268\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2584 - acc: 0.9277 - val_loss: 0.2668 - val_acc: 0.9268\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2582 - acc: 0.9281 - val_loss: 0.2671 - val_acc: 0.9267\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 0s - loss: 0.2580 - acc: 0.9280 - val_loss: 0.2667 - val_acc: 0.9268\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8704/10000 [=========================>....] - ETA: 0sSummary: Loss over the test dataset: 0.27, Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_prediction_ids = test_predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=6)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_view = 4\n",
    "\n",
    "idx = np.random.permutation(range(0,len(X_test)))[:n_view]\n",
    "\n",
    "imgs = X_test[idx,:].reshape(n_view, X_train_raw.shape[1], X_train_raw.shape[2])\n",
    "titles = test_prediction_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA80AAAD/CAYAAADL7DTOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XuUXGWZ7/HfQ6UxAjLILYERuY4HCREVDgwggoYZAjqA\nOqDAmIOiCAEEBIGQpptuSOLggBE4QRBEOAJrcBQvCAkXMRNEyHBzJUEOAwYRSMIlLjgCESje80dV\nY6e7n7e6dtW+VX8/a9WCrqd37ad36te7367q/VgIQQAAAAAAYLh18m4AAAAAAICiYtEMAAAAAICD\nRTMAAAAAAA4WzQAAAAAAOFg0AwAAAADgYNEMAAAAAICDRTMAAAAAAA4WzQAAAAAAOFg0AwAAAADg\nGJd3AygWM1tH0jWStpS0XNKxIYS38u0KQLPMbGtJ/yVpaf2uw0IIL+bYEoAEzGxzSTdJekPSm5KO\nCiGsyrcrAM0gx+XHK80Y6lOSfh9CmCLpUUmfzrkfAMn9KoTw8fqNBTNQTs+HEPYOIewn6f9IOibn\nfgA0jxyXHItmDLW9pIfr//+QpI/m2AuA1nzEzBaa2ay8GwGQTAghDPrwXZKW5dULgGTIcfmxaMZQ\nj0j6eP3/95f07hx7AZDcs5K2DyHsK2kzM/tU3g0BSMbMdjGzeyWdIOnBvPsB0DxyXG4smrGWEMLN\nktaY2R2S1pO0MueWACQQQngjhPBa/cObJO2SZz8Akgsh/DaE8PeSzpF0dt79AGgeOS43Fs0YJoTw\n9RDC/pJWS/pp3v0AaJ6ZbTDow30kPZ5XLwCSM7OuQR++LOmVvHoBkAw5Lj+uno21mNkESTdIqkq6\nM4Rwd84tAUjmI2Z2vmon5uWSunPuB0AyHzSzf1PtirtrJH0x534ANI8cl5yt/XfpAAAAAABgAG/P\nBgAAAADAwaIZAAAAAAAHi2YAAAAAABwsmgEAAAAAcKR29WwzO0HS6ZImSvqtpJNCCP81wudtIukA\nSU+qdjU5AMONl7SNpAUhhBez2uloc1z/XLIMNEaWgc5Q6CyTY2BURp/jEELbb5I+q1pAp0naUdLl\nqs383XSEzz1SUuDGjduobkemkdlWc0yWuXFr+kaWuXHrjFshsyxyzI1bM7eGOU5l5JSZ3SvpvhDC\nyfWPTdIfJV0cQrhgyOfuJenX0qclbVq/d76kqW3vKzn6iaOfuHb084KkH0vS3iGEe1puaRSayXG9\nTpabRj9xndhPGbPcif8O7UQ/jRWtp9b7+dCHxumhh+ZJBc0y5+Qk6CeuE/sZ/Tm57W/PNrMuSbtK\nmj1wXwghmNkdkvYcYZP6W0Y2lbRF/a7xg/6/COgnjn7i2tpPJm+xSpDjQb2R5dGjn7iO7qdEWe7o\nf4c2oJ/GitZT6/1ssEHXwP8WNcuck5tGP3Ed3U/DHKdxIbBNJVUkrRpy/yrV/v4CQPGRY6AzkGWg\nM5BlIEepXQisefNV+42BJD0j6QZJO0uanFtHQD6WSFo65L4yXcODLAM1nZLlgRxLZBlj0/AsL1tW\nlgE0nJOBmtbOyWksml+QVJU0Ycj9EySt9Debqr++xH6DpCNSaA0og8kafjJbIemKLJtImGOJLAMD\nOiXL5Bhj3fAsT5rUpUWLurNsgp+vgZa0dk5u+6/JQghvSHpA0pSB++oXKpgiaZQXSti53W21iH7i\n6CeuaP001p4cS8X72uknjn7iitZPY5yTs0A/jRWtp6L10xhZzgL9xI3tftJ6e/ZFkr5vZg9IWizp\nVEnrSfr+6DYv2ltG6CeOfuKK1s+otZhjqXhfO/3E0U9c0foZNc7JqaKfxorWU9H6GTWynCr6iRvb\n/aSyaA4h3Ghmm0rqV+1tIw9LOiCE8Hwa+wPQfuQY6AxkGegMZBnIT2oXAgshzJM0L63HB5A+cgx0\nBrIMdAayDOSjLJf+AwAAAAAgcyyaAQAAAABwsGgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAA\nAABwsGgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAAAABwsGgGAAAAAMDBohkAAAAAAAeLZgAA\nAAAAHCyaAQAAAABwsGgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAAAABwsGgGAAAAAMAxLu8G\nAAAAimX7aLX6tWlu7YKL2t1LzYGR2vPVPd3aJ1/6uVv7y8aXtNARAIwdvNIMAAAAAICDRTMAAAAA\nAA4WzQAAAAAAOFg0AwAAAADgYNEMAAAAAICj7YtmM+s1s7eG3B5p934ApIssA+VHjoHOQJaBfKU1\ncmqppCmSrP7xmyntB0C6yHIZHNDr1xb0uaWu509xa7/fOD5yZ8vu1W5t9hx/u7O/am6tcnE1uk/J\n/1oQRY6btmG0aqf6ta7IyCn/2S+dunu8owsWR/ZZ+Y1b+65t5tbWrX7CrX2usmu8IeSBLI+kt8ct\nzezt9rcLIfqw4yrfcGs9l/nb9XxlRvRx/X7i5Vn9s/xiH+fHtKW1aH4zhPB8So8NIDtkGSg/cgx0\nBrIM5CStv2n+OzN7xsyeMLMfmNlWKe0HQLrIMlB+5BjoDGQZyEkai+Z7JR0t6QBJx0naVtJ/mtn6\nKewLQHrIMlB+5BjoDGQZyFHb354dQlgw6MOlZrZY0h8kHS7pan/L+ZLGD7lvZ0mT29whUHRLVPuz\npcHWZN4FWQZalX+Wk+dYIsvAgOFZXrYs2wE0nJOBVrV2Tk7rb5rfFkJ4ycwek7RD/DOnStoi7XaA\nEpis4SezFZKuyKGXvyLLQLOKl+XR51giy8CA4VmeNKlLixZFLjKVMs7JQLNaOyen/msyM9tAtUCv\nSHtfANJDloHyI8dAZyDLQLbSmNP8TTP7qJltbWZ7SbpJ0huSbmj3vgCkhywD5UeOgc5AloF8pfH2\n7PdIul7SJpKel3S3pL8PIbyYwr4ApIcst90n3cr/rl4S3fK4y691a9dN9+czHuWPaNXszfztrol2\nE/f1v/FrlY9Fxope/J0W9goHOU7koWh1na2+F6nuG6n5g1hPW/x/o/s0/Q+3Vp3jv0P3WzP8fZ7y\n8V+4tc/p/Gg/tTUbMtThWY4PKa7O63drfZFzoFIaX9x/vF+rHD8nlX32yH/c2Az4nnl+rTK9N3lD\nY0waFwI7ot2PCSB7ZBkoP3IMdAayDOQr20v/AQAAAABQIiyaAQAAAABwsGgGAAAAAMDBohkAAAAA\nAAeLZgAAAAAAHGmMnELmpvqlf9nDr/W/7paqx4yP7vHxu/zaS5EL3+8+pepveGdKcwGALH2lxy1V\n36y4tTl+qVaP1GLfyG9+wa99tPpht7bvBffHG3rUHw/Sfc0j/naf8seGAOXxh0jNHw/XinCUf65f\nNSPZY9r/ioz5WchIKWQpNjRJGrfK//mxRw1OoBmLfyXZ6z/Br82s+msBSZpVWbfN3ZQXrzQDAAAA\nAOBg0QwAAAAAgINFMwAAAAAADhbNAAAAAAA4WDQDAAAAAOBg0QwAAAAAgIORU6Xhj4W4snqxW3uq\ncoP/kD/wS60NhfF7/f1tE93adpXjW9orkB3/OV49NzJWaovkezyyuplb2+5HK/0ND4+McotO6WAE\nHJCtyPgnSUuu8wN7fWS77SO1dSJjfFr9SQBop9B/nlvr6/Wfx929M1vYqV+qVL6R/HGzFvs6xjX4\nOiLHVn1j6+cEXmkGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAAAABwsGgGAAAAAMDBohkAAAAA\nAAcjpwrj89Fq9aId3NqcyNgYizxmz7l+bf2vPh/tZ83Gt0b2+Ya/z//4UmSnvX7tlbF1WXsU3CT/\nuWrH+mNa3h15yBurt0R32V15IFKNjYaJfRcAkC1/9subz8dHPH3DnzoX/WHun/4YKW7FWCmURIjM\nTerzn8fn962bQjOS1JNwu8g5uTf+mL190TmRycSOq6SZPf7IrlmpHdti4pVmAAAAAAAcLJoBAAAA\nAHCwaAYAAAAAwMGiGQAAAAAAB4tmAAAAAAAcTS+azWwfM/uZmT1jZm+Z2cEjfE6/mT1rZq+a2e1m\n5l/6GUDmyDHQGcgyUH7kGCi+JCOn1pf0sKSrJP14aNHMzpR0oqRpkp6UdL6kBWb2/hDC68lb7QSb\nu5U/vf7+6Jb9kau67xzZ7rBHqm6tb6fIGKdzL432ExO7eP1Bh/3Irf36nz/s1vauHJK4H4yIHLfi\nan9kxOzd/c1O39CvnVC5r4WGMIaR5TLZ1x9X98vN4uOfYj+wff0qv1bZyv85ID6uDhkix6kp0djF\n/ngee+f5tb7pbe5lQIkOX9qaXjSHEOZLmi9JZjbSoTxZ0nkhhJvrnzNN0ipJh0q6MXmrANqFHAOd\ngSwD5UeOgeJr6980m9m2kiZKunPgvhDCy5Luk7RnO/cFIB3kGOgMZBkoP3IMFEO7LwQ2UbV3564a\ncv+qeg1A8ZFjoDOQZaD8yDFQAEn+pjkl8yWNH3LfzpIm59ALkKclkpYOuW9NHo0kRJaBGrIMdIbh\nWV62rCwDaMgxUNPaObndi+aVqv3J+ASt/RuxCZIeim86VdIWbW4HKKPJGn4yWyHpiqwaaCHHElkG\nBpBloDMMz/KkSV1atKg7i52TY6AtWjsnt/XXZCGE5aqFe8rAfWa2oaQ9JN3Tzn0BSAc5BjoDWQbK\njxwDxdD0K81mtr6kHfTXi5BvZ2a7SFodQvijpLmSus3scdUui3+epKcl/bQtHQNoGTkGOgNZBsqP\nHAPFl+Tt2btJuku1ixIESRfW779G0hdDCBeY2XqSLpe0kaRFkg5kjpwkHeZW5q57QnTLHWOPOi8y\ng3Gn2My37Iev7ajfubXfVB6ObPndBo98c6J+xjByHPXuaLX6c/9NOrMj242/MpLVw5mXikTIctH0\n9ril6p8qbm3uwvjDTovUKs8xi7nkyPFYEfv+MMH//iClOIsZo5JkTvNCNXhbdwjhXEnnJmsJQNrI\nMdAZyDJQfuQYKL6yXPoPAAAAAIDMsWgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAAAABwJBk5\nhYQ2eDn4xQ3j266sftkvVoo2TmIvt/Ktzxzg1l6PHIPTX368lYaA5sw+KVqec/Ypbu3sLfztzjnc\nH7kGoEz883nVImOlLk7yiDVbyh9VoxlF+zkA6HR+YmdW/UlglYr//SGPFPfOi9cr63Rl00gJ8Eoz\nAAAAAAAOFs0AAAAAADhYNAMAAAAA4GDRDAAAAACAg0UzAAAAAAAOFs0AAAAAADgYOZUl80uNRk2c\nWrksUj0vSTct+Ux1e7d247H+WKm+7/mP+c43T/CLlUdH0xbQFruceV+0Hs72a91Pd/vFyg8TdiRJ\n20RqH4rUPpBsd7c2qP9t0l1eFKn9vwY7BYrv5+f8g18893a3FPkRQZK0sPoLt7Zv5Z7IlrMbPDJQ\ncr3+OLaZPTPdWqXyjTS6kfypUg1znobYGqMyPTLKTlI+HRcTrzQDAAAAAOBg0QwAAAAAgINFMwAA\nAAAADhbNAAAAAAA4WDQDAAAAAOBg0QwAAAAAgIORUxn687abJt/4wcjvN+7sTfSQW5/mj3E6L0TG\n5kh6vNLn1voj270rUjv9zIsj1dijAm3WYMJCrHz+HbPc2nPVb7u171731eg+w71+7dXv+7W5r/q1\nNyL76zow2k5iZ3/LP3q7fXVhdNuHKne2ux0gIf95fOi4Pd3avtVT3NrUyieie/zluAfcWnW38W5t\nwn3L3doLlfdG9uif54Hs+YOTqhP9GU99BRv/VDwchdHilWYAAAAAABwsmgEAAAAAcLBoBgAAAADA\nwaIZAAAAAAAHi2YAAAAAABxNL5rNbB8z+5mZPWNmb5nZwUPqV9fvH3y7pX0tA2gVOQY6A1kGyo8c\nA8WX5JXm9SU9LGm6/Ou/3yppgqSJ9dsRiboDkBZyDHQGsgyUHzkGCq7pOc0hhPmS5kuSmXnDvf4S\nQni+lcY60gt3uKVPN9r2w+1/J/16Z/i1f7k2vu3N1Y+5tQcqd7m1U6/yH/P0Y5jFnBVyLMVmPj7Y\nvVd0yzmx2gH+424jfxbz7OgepTcjtZ5t/drUx9/v1g546zZ/Q//LkCStPmNLt3bihRe4Nfv6DLf2\nmXEfje7z/m38eZKVJ78T2fLZ6OOWGVkuoOA/TxeO28SvdVejD3ta3/lubfW6vW7trHHbuLVTD41k\n6rV4P1rAHOd2IcetiU0aZgpx/BhU58VzXJnuf28Za9L6m+b9zGyVmT1qZvPMbOOU9gMgPeQY6Axk\nGSg/cgzkqOlXmkfhVkk/krRc0vaqvShzi5ntGUJo8NoFgIIgx0BnIMtA+ZFjIGdtXzSHEG4c9OEy\nM1si6QlJ+0ny37er+ZLGD7lvZ0mT29sgUHhLJC0dct+aTDtInmOJLAMDyDLQGYZnedmy7AbQkGOg\nHVo7J6fxSvNaQgjLzewFSTsoGuypkrZIux2gBCZr+MlshaQrcuilZvQ5lsgyMIAsA51heJYnTerS\nokXduXRDjoEkWjsnp/5rMjN7j6RNVOsKQAmRY6AzkGWg/MgxkL2mX2k2s/VV+83WwMXYtjOzXSSt\nrt96Vfu7i5X1z/tXSY9JWtCOhgG0jhwDnYEsA+VHjoHiS/L27N1UeytIqN8urN9/jWrz5T4gaZqk\njVSb87FAUk8I4Y2Wuy29e9zKh7QwuuV337rMrW2i1W7ttHChW/t9pcutzZh2fbSf+Ufd6dZiV6To\nPfosv3hMdJdoL3L8Dn+MwrWz0xl/dtI7/NpZr/5bdNvv/Oupbu3cs+/2N6z4WZWujO4zqUvnRmp2\nuVuzn34p+ri7H1Jxawur/uPuW/lK5FFLP46KLJdJuNWvnR+pSbpwll+76CB/PNQffuHnZu5P/DP2\nTfK3k6RPqSdaR1PIcUOR8Wgr/ed/T4Pn8VjXPz3vDsojyZzmhYq/rXtq8nYAZIEcA52BLAPlR46B\n4svu0n8AAAAAAJQMi2YAAAAAABwsmgEAAAAAcLBoBgAAAADAwaIZAAAAAABHkpFTSEHQL6P1L63z\nvoSP/B+JtgqxuVGSHqr82q31TvK3q1QOiTzq/PhOgXZ6/YduaXX1uOim76p8x62duMzfrjLJH4uh\nSqMxV0nHYPljOnIR/BFP4ZD417h/ZDLfrHEP+cVNvuzXXuyL7hMojMh5OdziZ+eyardb27Ryvlt7\nIjpAUvrb6pFu7ZlKfGwl0E6hz3/+90VGo82sxid2zer386G+7M8dvYl/DvA1+HFf6vXHc+ZxDPLE\nK80AAAAAADhYNAMAAAAA4GDRDAAAAACAg0UzAAAAAAAOFs0AAAAAADhYNAMAAAAA4GDkVGlkOzZm\nu+o/R+uvRcbjWGxaz0mMlUJBhN+5pdPGvT++7cn+6KiTJ8VGMLR/XERH2c4fDSJJrx4YObaN5uQB\nnexv/OzM2qHi1r4VechGP3U8deiObq0SGfMDZMt/Js+qdDXYNnbOTuPncs5jRcYrzQAAAAAAOFg0\nAwAAAADgYNEMAAAAAICDRTMAAAAAAA4WzQAAAAAAOFg0AwAAAADgYOTUmOZf2v6/j/lAdMvYUJ2D\np/+7XzzpkQY9AQUQVsfr3856DEUn8cfUXP3YEdEt5/qTc3T2Dn7tnMcfaNQUUALbu5XqF/1wXHBR\nsr01Gn5T+fk5yR4YaLdef8TZzJ5utzarsm4a3cRFeq1OiJzkJPVPb3czUu+8eL1/euwn/rGFV5oB\nAAAAAHCwaAYAAAAAwMGiGQAAAAAAB4tmAAAAAAAcLJoBAAAAAHA0tWg2sxlmttjMXjazVWZ2k5m9\nb4TP6zezZ83sVTO73cwi1zUFkDWyDJQfOQY6A1kGiq/ZV5r3kXSJpD0k7S+pS9JtZvbOgU8wszMl\nnSjpWEm7S3pF0gIzy+G67gAcZBkoP3IMdAayDBRcU3OaQwgHDf7YzI6W9JykXSXdXb/7ZEnnhRBu\nrn/ONEmrJB0q6cYW+0VbnepWFnw/NodW6nm3XxtXWZ60IWSELCNVO0TmUH7Qn0M5Jz6iMjo3tjKv\n6hf/Mf79rKzIcaeJT0aufm2aW5sbmcUc+0EvNlX+lN2j7WjGYmbStwtZHoXIfOOevsjJI/Ltf2Z1\nRnSXiec4x2YxT/R77T8+2e4aiZ47p/u9Ym2t/k3zRqr9W6yWJDPbVtJESXcOfEII4WVJ90nas8V9\nAUgPWQbKjxwDnYEsAwWTeNFsZiZprqS7QwiP1O+eqFrIVw359FX1GoCCIctA+ZFjoDOQZaCYmnp7\n9hDzJO0kae/2tDJf0vgh9+0saXJ7Hh4ojSWSlg65b02aOyTLQCoyzXKbcyyRZWDA8CwvW5baABrO\nyUAqWjsnJ1o0m9mlkg6StE8IYcWg0krV/kRmgtb+bdgESQ/FH3WqpC2StAN0mMkafjJbIemKtu+J\nLANpyibL6eRYIsvAgOFZnjSpS4sWdbd1L5yTgTS1dk5u+tdk9UAfIuljIYSnBtdCCMtVC/aUQZ+/\noWpXA7yn2X0BSA9ZBsqPHAOdgSwDxdbUK81mNk/SEZIOlvSKmU2ol14KIQy8vj1XUreZPS7pSUnn\nSXpa0k/b0jGAlpFloPzIMdAZyDJQfM2+Pfs41S5E8Ksh939B0rWSFEK4wMzWk3S5alf/WyTpwBDC\n6621inb7RPU2t3Zfg9EvU56IFDd+LVlDyBJZRgO90eqHq4vc2sINI2OlHvcf870NOnqtepRfrMxt\nsHVHIse5meqXfujParr/0zu7tS0rv4vu8ZuRsVJdke1if7n6SPXLbq1SOTTaj7S4QR1NIMsNRq7N\n7PHfCm99yR62Mm5OdJ/d1bPcWt/l3/Br0/2G0hqAGDt6/b2RkYx9nTmSMQ3Nzmke1du5QwjnSjo3\nQT8AMkCWgfIjx0BnIMtA8aV26T8AAAAAAMqORTMAAAAAAA4WzQAAAAAAOFg0AwAAAADgYNEMAAAA\nAICj2ZFTKB3/n/hnV33WrcWu3i9J7zgjcvn61C6oD4xlx0Rqd/qlL01zS9Xj/dFQ1+wa/y6wOjKW\n7gnza69FRnhMq+wU3acqkXlVejm+LcYofzTUR6r+c2bhcwdEH3XVFn4+rj/M325h9FGT+3wkj5tP\njZyvK7GcM1IKxWEWH0mV6DEbPOQ6FX+sVOwn3cgpsCWMlcoXrzQDAAAAAOBg0QwAAAAAgINFMwAA\nAAAADhbNAAAAAAA4WDQDAAAAAOBg0QwAAAAAgIORU53us2e7pfO+co5b6/1a/GH7L5qTtCMAI3mo\nJ1p+a11/pkyY7W83+8ovuLX+K/3tuqLdSP8YqX3w2KQjbp5osNe0Bnmgcz3nVu6e9Em39s1H448a\ny0fSZ2mjgTqvVc9wa5tXIt8EfpHHcBygWfHn4vmVdd1ad2SUYSUyNqq1jtqv0feAauTrVIWxUmnj\nlWYAAAAAABwsmgEAAAAAcLBoBgAAAADAwaIZAAAAAAAHi2YAAAAAABwsmgEAAAAAcLBoBgAAAADA\nwZzmjuBPdrv3+l3c2q3/7j/i1d/8XHyXF73RqCkAzTgpPhHyurv92h8i283cza89vXhjt3ZMuCra\nT/d7D/aLlzMXFkXxoF961K+dpfjc9Fz4o9olnZdVF0A+gn/uOH/cO9ya9VbdWnVCNFTqm964rZHE\n5i3HZi3PisyiltTgewDSxivNAAAAAAA4WDQDAAAAAOBg0QwAAAAAgINFMwAAAAAAjqYWzWY2w8wW\nm9nLZrbKzG4ys/cN+ZyrzeytIbdb2ts2gFaQZaD8yDHQGcgyUHzNvtK8j6RLJO0haX9JXZJuM7N3\nDvm8WyVNkDSxfjuixT4BtBdZBsqPHAOdgSwDBdfUyKkQwkGDPzazoyU9J2lXSYMHovwlhPB8y92h\nZWb+he83j2z3pcp1DR45NlIGRUeWC+juvmj58wlH4HTfHylGf236cINHblRH2shxmhiNhuyQ5RZF\nZjyFPv/n1XXyGC0XHRvF950ia/VvmjdS7am6esj9+9XfXvKomc0zM38YKIAiIMtA+ZFjoDOQZaBg\nmnqleTAzM0lzJd0dQnhkUOlWST+StFzS9pLmSLrFzPYMIcTmfQPIAVkGyo8cA52BLAPFlHjRLGme\npJ0k7T34zhDCjYM+XGZmSyQ9IWk/SXf5Dzdf0vgh9+0saXILLQJltETS0iH3rUlzh2QZSEWmWW5z\njiWyDAwYnuVly1IbQMM5GUhFa+fkRItmM7tU0kGS9gkhrIh9bghhuZm9IGkHRUM9VdIWSdoBOsxk\nDT+ZrZB0Rdv3RJaBNGWT5XRyLJFlYMDwLE+a1KVFi7rbuhfOyUCaWjsnN71orgf6EEn7hhCeGsXn\nv0fSJvWuABQEWQbKjxwDnYEsA8XW7JzmeZKOknSkpFfMbEL9Nr5eX9/MLjCzPcxsazObIuknkh6T\ntKDdzQNIhiwD5UeOgc5AloHia/aV5uNUu5rfr4bc/wVJ10qqSvqApGmqXfnvWdXC3BNCeKOlTpFI\n7OL1x/X6tRMil+hHRyDLpcMoCgxDjoHOQJZzwXkVo9fsnOboK9MhhDWq/fEEgAIjy0D5kWOgM5Bl\noPhSu/QfAAAAAABlx6IZAAAAAAAHi2YAAAAAABwsmgEAAAAAcLBoBgAAAADA0ezIKRSSf8n83df5\nTGS7WA0AAAAAUNBXmpfk3cAQ9BNHP3FF6ydLRfva6SeOfuKK1k9WivZ1009c0fqRitdT0frJStG+\nbvqJo5+4bPsp6KJ5ad4NDEE/cfQTV7R+slS0r51+4ugnrmj9ZKVoXzf9xBWtH6l4PRWtn6wU7eum\nnzj6icu2n4IumgEAAAAAyB+LZgAAAAAAHCyaAQAAAABwFOHq2eNr/3lh0F1rJK3IoxdHmfsJCffh\nX5F7uDIfnyy0o5+38zG+xQdKE1luGv3EdWI/ZcxyJ/47tBP9NFa0nlrv589/fvtH6KJmmXNy0+gn\nrhP7Gf052UJIuqhqDzM7UtJ1uTYBlMdRIYTr825iJGQZaApZBjpDIbNMjoGmNMxxERbNm0g6QNKT\nqv3KAMBw4yVtI2lBCOHFnHsZEVkGRoUsA52h0Fkmx8CojDrHuS+aAQAAAAAoKi4EBgAAAACAg0Uz\nAAAAAABpn+lLAAAFTElEQVQOFs0AAAAAADhYNAMAAAAA4GDRDAAAAACAo1CLZjM7wcyWm9lrZnav\nmf3PHHvpNbO3htweyXD/+5jZz8zsmfq+Dx7hc/rN7Fkze9XMbjezHfLqx8yuHuF43ZJSLzPMbLGZ\nvWxmq8zsJjN73wifl+XxadhTlscob0XJMjluvqexnGVyPBxZfnv/hcpykXJc3x9ZLjByvFYPZNnv\nhRxHFGbRbGaflXShpF5JH5L0W0kLzGzTHNtaKmmCpIn120cy3Pf6kh6WNF3SsLlgZnampBMlHStp\nd0mvqHa81s2jn7pbtfbxOiKlXvaRdImkPSTtL6lL0m1m9s6BT8jh+DTsqS6rY5SbAmaZHDfRU91Y\nzTI5HoQsr6VoWS5SjiWyXFjkeBiy7CPHMSGEQtwk3Svp24M+NklPSzojp356JT2Y93Gp9/KWpIOH\n3PespFMHfbyhpNckHZ5TP1dL+nFOx2fTek8fKcLxifSU2zHK+N+jMFkmx4l6IsvxfsZEjutfK1ke\nuZdCZbloOa7vnywX5EaOo/2Q5Xg/5HjQrRCvNJtZl6RdJd05cF+oHYk7JO2ZV1+S/q7+doknzOwH\nZrZVjr28zcy2Ve03KYOP18uS7lO+x2u/+tsnHjWzeWa2cUb73Ui1386tlgpzfNbqaZC8jlEmCppl\nctw8sjxCP4N0dI4lstyMAjxPPXk+T8lyAZDj5hTgeerhnDxCP4NkcnwKsWhW7TcHFUmrhty/SrV/\nnDzcK+loSQdIOk7StpL+08zWz6mfwSaq9qQp0vG6VdI0SR+XdIakfSXdYmaW5k7rjz9X0t0hhIG/\ni8n1+Dg9STkdo4wVLcvkuHlk2e9HGhs5lshyM4qY5dyep2S5UMhxc8hyHTkebly7H7BThBAWDPpw\nqZktlvQHSYer9lYADBJCuHHQh8vMbImkJyTtJ+muFHc9T9JOkvZOcR/NGrGnHI/RmEWOm0eW30aO\nC4QsNyfn5ylZxojIcfM4J78t9xwX5ZXmFyRVVfsj7sEmSFqZfTvDhRBekvSYpFSvbDtKK1X7m5Qi\nH6/lqv27pnnFwUslHSRpvxDCikGl3I5PpKdhsjhGOSh0lslx88ZilsmxJLLcjMJnOavnKVkuHHLc\nHLIscuwpxKI5hPCGpAckTRm4r/6y+hRJ9+TV12BmtoFq/wDRf6ws1J8QK7X28dpQtavLFeV4vUfS\nJkrpeNUDdIikj4UQnhpcy+v4xHpyPj/VY5SHomeZHDdvrGWZHNeQ5dErQ5azeJ6S5eIhx80hy+Q4\nKourjY3mptpbM15V7X3pO0q6XNKLkjbLqZ9vSvqopK0l7SXpdtXes79JRvtfX9Iukj6o2pXiTql/\nvFW9fkb9+PyTpMmSfiLpvyWtm3U/9doFqoVma9XCdL+k30nqSqGXeZL+pNql6CcMuo0f9DlZH59o\nT1kfozxvRcoyOW6up7GeZXI87HiQ5b/uv1BZLlKO6/2Q5YLeyPGwHsiy3ws5jvWT1ZN0lAdnuqQn\nVbt0+W8k7ZZjLzeodkn+1yQ9Jel6SdtmuP996+GpDrl9b9DnnKvapd9flbRA0g559CNpvKT5qv32\naY2k30u6LK1vyE4fVUnThnxelscn2lPWxyjvW1GyTI6b62msZ5kcj3hMyHIoXpaLlON6P2S5wDdy\nvFYPZNnvhRxHblbfKQAAAAAAGKIQf9MMAAAAAEARsWgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCya\nAQAAAABwsGgGAAAAAMDBohkAAAAAAAeLZgAAAAAAHCyaAQAAAABwsGgGAAAAAMDBohkAAAAAAMf/\nBzRrELWBbJmDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ef92cf890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(imgs, titles=titles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
